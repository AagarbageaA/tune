# -*- coding: utf-8 -*-
"""fine_tune.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rNIRsYzmWqE2LDe3LtmrM6EpgiUjLR2Y
"""

!pip install -i https://pypi.org/simple/ bitsandbytes
!pip install accelerate
!pip install pyarrow==14.0.1
!pip install requests==2.31.0
!pip install datasets==2.19.0
!pip install peft
!pip install trl
!pip install wandb
!pip install peft

# 先登入huggingface
from huggingface_hub import login
# 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
login()

import torch
from datasets import load_dataset
from transformers import(
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    TrainingArguments,
    pipeline,
    logging,
    TextStreamer,
    Trainer
)
from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training
from trl import SFTTrainer
import os,wandb
# print(torch.cuda.is_available())
# print(torch.cuda.device_count())

# 載入模型和Tokenizer
model_name="taide/Llama3-TAIDE-LX-8B-Chat-Alpha1"

# 量化成4bit
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type = "nf4",
    bnb_4bit_compute_dtype = torch.float16,
    bnb_4bit_use_double_quant = False
)

# 加載模型
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config = bnb_config,
    device_map = {"":0}
)

model = prepare_model_for_kbit_training(model)

# 加載tokenizer
tokenizer = AutoTokenizer.from_pretrained(
    model_name,
    add_eos_token=True,
    add_bos_token=True,
)
tokenizer.pad_token = tokenizer.eos_token


# 加載數據集
dataset_name="/content/data"
dataset = load_dataset(dataset_name, split = "train")
#print(dataset["instruction"][0])

# 設置wandb
wandb.login(key = "95a5efe7670b48139b6aa90f59088a70ab60766e")
run = wandb.init(
    project = "fine_tune",
    job_type = "training",
    config={
        #"conv_1": 32,
        #"activation_1": "relu",
        #"kernel_size": (3, 3),
        #"pool_size": (2, 2),
        #"dropout": 0.3,
        #"conv_2": 64,
        #"activation_out": "softmax",
        #"optimizer": "adam",
        "loss": "sparse_categorical_crossentropy",
        "metric": "accuracy",
        #"epoch": 6,
        #"batch_size": 32
    }

)

def print_trainable_parameters(model):
    trainable_params = 0
    all_params = 0
    for _,param in model.named_parameters():
        all_params += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    print(f"訓練參數量 : {trainable_params}, 全部參數量 : {all_params}, 訓練參數量占比% : {100*(trainable_params/all_params):.2f}")

# Lora和訓練超參的設定
peft_config = LoraConfig(
    r = 8, #要自己調，每個訓練集不一樣.
    lora_alpha = 16, #r的2倍
    lora_dropout = 0.05,
    bias = "none",
    task_type = "CAUSAL_LM",
    target_modules= ["q_proj" , "k_proj" , "v_proj" , "o_proj " , "gate_proj" , "up_proj" , "down_proj" , "embed_tokens" , "lm_head"]
    #不一定要全選
)

training_arguments = TrainingArguments(
    # Output directory where the results and checkpoint are stored
    output_dir = "./results",
    # Number of training epochs - how many times does the model see the whole dataset
    num_train_epochs = 5, #Increase this for a larger finetune
    # Enable fp16/bf16 training. This is the type of each weight. Since we are on an A100
    # we can set bf16 to true because it can handle that type of computation
    bf16 = False,#True對GPU要求較高
    # Batch size is the number of training examples used to train a single forward and backward pass.
    per_device_train_batch_size = 2,
    # Gradients are accumulated over multiple mini-batches before updating the model weights.
    # This allows for effectively training with a larger batch size on hardware with limited memory
    gradient_accumulation_steps = 2,
    # memory optimization technique that reduces RAM usage during training by intermittently storing
    # intermediate activations instead of retaining them throughout the entire forward pass, trading
    # computational time for lower memory consumption.
    gradient_checkpointing = True,
    # Maximum gradient normal (gradient clipping)
    max_grad_norm = 0.3,
    # Initial learning rate (AdamW optimizer)
    learning_rate = 2e-4,
    # Weight decay to apply to all layers except bias/LayerNorm weights
    weight_decay = 0.001,
    # Optimizer to use
    optim = "paged_adamw_8bit",
    # Number of training steps (overrides num_train_epochs)
    max_steps = -1,# -1表示沒有限制
    # Ratio of steps for a linear warmup (from 0 to learning rate)
    warmup_ratio = 0.03,
    # Group sequences into batches with same length
    # Saves memory and speeds up training considerably
    group_by_length = True, #用來提高訓練效率
    # Save checkpoint every X updates steps
    save_steps = 100,
    # Log every X updates steps
    logging_steps = 30,
    lr_scheduler_type = "linear",
    report_to = "wandb"
)

# 模型微調
trainer = SFTTrainer(
    model = model,
    train_dataset = dataset,
    peft_config = peft_config,
    tokenizer = tokenizer,
    dataset_text_field = "text",
    args = training_arguments,
    packing = False
)

trainer.train()
model = get_peft_model(model, peft_config)
print_trainable_parameters(model)
trainer.model.save_pretrained("/7_13/final_model/")
wandb.finish()
model.config.use_cache = True
model.eval()

# base模型測試
def stream(user_input):
  device = "cuda:0"
  system_prompt = ""
  B_INST, E_INST = "### Instruction:\n", "### Response:\n"
  prompt = f"{system_prompt}{B_INST}{user_input.strip()}\n\n{E_INST}"
  inputs = tokenizer([prompt], return_tensors = "pt").to(device)
  streamer = TextStreamer(tokenizer, skip_prompt = True, skip_special_tokens = True)
  _ = model.generate(**inputs, streamer = streamer, max_new_tokens = 1024)

stream("以下會提供一篇可能有提到關鍵字\"地層下陷\"的文章，閱讀以下文章，盡可能仔細地回答以下問題。說明這起有提到\"地層下陷\"的事件的起因是甚麼?是自然發生的還是人為的?你又是從哪些關鍵字句下去判斷的?\n文章：台北市和平東路三段一處工地旁道路，凌晨驚傳地層下陷！連上頭停放的轎車，也跟著下墜，卡在1.5公尺深的大洞裡，還壓迫到自來水管線，導致破裂流出大灘積水。居民聽到巨響，趕緊報案處理，也透露昨（4）天下午路面就出現裂縫。當時建管處會勘，把原因歸咎於地震，但他們無法接受，質疑是工地施工導致。工地圍牆外一大塊路面下陷坍塌，上頭停放的轎車也跟著地層向下墜受困，一旁管線流出的積水也逐漸淹進車頭，就怕轎車泡水拋錨，警方請來吊車協助車輛脫困，但一移開景象更驚人，道路側邊就像破了大洞，凹陷1.5公尺坑洞，面積也有30平方公尺。4日下午北市和平東路三段這處工地外的道路開始出現裂縫，建管處與工地人員前來會勘，歸咎於3日地牛翻身，還貼出警示牌提醒民眾，沒想到5日凌晨地層整個塌陷，不過這答案附近居民無法接受。有人說地震前就已經看到裂縫，但剛好遇上地震，相關單位就把責任推給地震；還有住民透露，這處工程在打地基時，自己在家中就曾感覺到震動，就像地震一樣，質疑根本是施工問題釀成地層下陷；居民的滿腔怒火恐怕是建管處在修補道路外，下個得要面對的難題。")

stream("你好")

from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel, PeftConfig
import torch

model_name = "taide/Llama3-TAIDE-LX-8B-Chat-Alpha1"

# 加载基础模型
base_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    low_cpu_mem_usage=True,
    return_dict=True,
    torch_dtype=torch.float16,
    device_map={"": 0}
)

# 从本地路径加载适配器模型
local_model_path = "/content/sample_data/final_model"
new_model = PeftModel.from_pretrained(base_model, local_model_path)

# 如果需要合并模型权重并卸载适配器
merged_model = new_model.merge_and_unload()

# 确保设置 tokenizer 的 pad_token
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

tokenizer.padding_side = "right"

from google.colab import drive
drive.mount('/content/drive')